{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhVi_jp9DOpL"
   },
   "source": [
    "# MPI (Message-Passing Interface)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "* Mechanism of data exchange between processes\n",
    "* Two types of communication: \n",
    " * **point-to-point**: between two processes \n",
    " * **collective**: between multiple processes\n",
    "* Typically only one program, branching depending on the process \n",
    "* Using the mpi4py Python library \n",
    "\n",
    "An mpi4py tutorial: \n",
    "* https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
    "\n",
    "\n",
    "Install mpi4py:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89pnn-HgDpjV"
   },
   "source": [
    "## A basic example (no data exchange)\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wk1x92wcDOpR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from process  0\n"
     ]
    }
   ],
   "source": [
    "# mpi.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank() # index of the current process \n",
    "print (\"hello from process \", rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UptBtCQoDOqK"
   },
   "source": [
    "## Point-to-point communication of two processes\n",
    "\n",
    "### Example: computing $\\pi$ with MPI \n",
    "\n",
    "$$\\pi=\\sqrt{6\\sum_{n=1}^{\\infty}\\frac{1}{n^2}}$$\n",
    "\n",
    "**Exercise:** Theoretically estimate the error resulting if we truncate the series at $N$ terms.  \n",
    "\n",
    "Without MPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJeSQbxbDOqU",
    "outputId": "eb817b4a-7a87-466e-a32f-6fddae917399"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(1,200000)\n",
    "print (np.sqrt(6*np.sum(1./(a*a))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrFHlcHIDOqz"
   },
   "source": [
    "### Functions ``send()``, ``recv()``\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 2 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NFhjMInDOq4"
   },
   "outputs": [],
   "source": [
    "# Evaluate the sum of 2M terms by splitting into two groups of M terms.\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank() \n",
    "\n",
    "M = 100\n",
    "def getPartialSum(start, end):\n",
    "    a = np.arange(start, end)\n",
    "    return np.sum(1./(a*a))\n",
    "\n",
    "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
    "print ('Process', rank, 'found partial sum from term', 1+rank*M, 'to term', 1+(rank+1)*M-1, ': ', s )\n",
    "\n",
    "# process 1 sends its partial sum to process 0\n",
    "if rank == 1:\n",
    "    comm.send(s, dest=0) \n",
    "    \n",
    "# process 0 receives the partial sum from process 1, adds to its own partial sum\n",
    "# and outputs the result    \n",
    "elif rank == 0: \n",
    "    s_other = comm.recv(source=1)\n",
    "    s_total = s+s_other\n",
    "    print ('total partial sum =', s_total)\n",
    "    print ('pi_approx =', np.sqrt(6*s_total))\n",
    "    \n",
    "print ('Process', rank, 'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyaN4fOCFnfN",
    "outputId": "8a91a7b6-b6c5-486f-8e36-b8a9c15dfdf1"
   },
   "outputs": [],
   "source": [
    "!mpirun --allow-run-as-root -n 2 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-wSXWdXH1pY"
   },
   "source": [
    "**Exercise:** Perform the same computation in a \"ping-pong\" manner: one process sums only even terms, the other only odd terms; after adding a new term the process sends the current result to the other process.  \n",
    "\n",
    "## Collective communication\n",
    "\n",
    "Perform efficient (fast, load-balanced) collective operations (e.g., summations) involving multiple processes. \n",
    "\n",
    "<img src='https://materials.jeremybejarano.com/MPIwithPython/_images/fastSum.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5eg3eZdDOr1"
   },
   "source": [
    "### Function ``gather()``\n",
    "\n",
    "Pass data from all processes to the chosen process.\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 5 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGNWAOcPDOr-"
   },
   "outputs": [],
   "source": [
    "# Evaluate the sum of MN terms by splitting into M groups of N terms.\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size() # total number of processes\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "M = 100\n",
    "def getPartialSum(start, end):\n",
    "    a = np.arange(start, end)\n",
    "    return np.sum(1./(a*a))\n",
    "\n",
    "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
    "\n",
    "partialSums = comm.gather(s, root=0)\n",
    "print ('partialSums gathered at process %d:' %(rank), partialSums) \n",
    "\n",
    "if rank == 0:\n",
    "    print ('pi_approx =', np.sqrt(6*np.sum(partialSums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "lWUfMyhSGG6D",
    "outputId": "ebd9d3f1-22b9-496e-d959-0d5bce191163"
   },
   "outputs": [],
   "source": [
    "!mpirun --allow-run-as-root -n 5 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCXH5iJXICPd"
   },
   "source": [
    "### Function ``bcast()``\n",
    "\n",
    "Pass data from the chosen process to all other processes.\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mw2GT71eDOsY"
   },
   "outputs": [],
   "source": [
    "# basic usage of bcast()\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    some_data = {0: 'abcd', 1:1234}\n",
    "else:\n",
    "    some_data = None\n",
    "    \n",
    "print (\"I'm process\", rank, '; data before broadcasting:', some_data)\n",
    "data = comm.bcast(some_data, root=0)\n",
    "print (\"I'm process\", rank, '; data after broadcasting:', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "vRlOFLVFGL-5",
    "outputId": "5199ffac-620b-49eb-af84-577426e4113b"
   },
   "outputs": [],
   "source": [
    "!mpirun --allow-run-as-root -n 3 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNm6buMYITTW"
   },
   "source": [
    "### Functions ``scatter()``, ``reduce()``\n",
    "\n",
    "* ``scatter()``: distribute data from one source to all processes\n",
    "* ``reduce()``: combine data from all processes using a collective operation like `sum` or `max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTGFw1YLDOsq"
   },
   "outputs": [],
   "source": [
    "# Evaluate the sum of N terms by scattering them to N processes.\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data2scatter = [a*a for a in range(1,size+1)]\n",
    "else:\n",
    "    data2scatter = None\n",
    "    \n",
    "data = comm.scatter(data2scatter, root=0)\n",
    "\n",
    "print ('Data at process', rank, ':', data)\n",
    "\n",
    "b = 1./data\n",
    "\n",
    "partialSum = comm.reduce(b, op = MPI.SUM, root = 0)\n",
    "\n",
    "print ('Partial sum at process', rank, ':', partialSum)\n",
    "\n",
    "if rank == 0:\n",
    "    result = np.sqrt(6*partialSum)\n",
    "    print ('Pi_approx:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "S5NWqw3HGRTX",
    "outputId": "23e0387c-1e51-4cdf-897d-28adce5c130f"
   },
   "outputs": [],
   "source": [
    "!mpirun --allow-run-as-root -n 5 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Tmm87fhIdwU"
   },
   "source": [
    "## Example: parallel scalar product\n",
    "* Generate two random vectors $\\mathbf x$ and $\\mathbf y$ at the root process. Goal: compute their scalar product $\\langle\\mathbf x,\\mathbf y\\rangle$ \n",
    "* Divide $\\mathbf x$ and $\\mathbf y$ into chunks and scatter them to the other processes\n",
    "* Compute scalar products between chunks at each process\n",
    "* Obtain $\\langle\\mathbf x,\\mathbf y\\rangle$ by reducing (summing) local scalar products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiixH1tCDOs9"
   },
   "outputs": [],
   "source": [
    "#\"to run\" syntax example: mpirun -n 10 python mpi.py 4000000\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "#read from command line\n",
    "N = int(sys.argv[1])    #length of vectors\n",
    "\n",
    "#arbitrary example vectors, generated to be evenly divided by the number of\n",
    "#processes for convenience\n",
    "\n",
    "x = np.random.rand(N) if comm.rank == 0 else None\n",
    "y = np.random.rand(N) if comm.rank == 0 else None\n",
    "\n",
    "#initialize as numpy arrays\n",
    "dot = np.array([0.])\n",
    "local_N = np.array([0])\n",
    "\n",
    "#test for conformability\n",
    "if rank == 0:\n",
    "                if (N != y.size):\n",
    "                                print (\"vector length mismatch\")\n",
    "                                comm.Abort()\n",
    "\n",
    "                #currently, our program cannot handle sizes that are not evenly divided by\n",
    "                #the number of processors\n",
    "                if (N % size != 0):\n",
    "                                print (\"the number of processors must evenly divide n.\")\n",
    "                                comm.Abort()\n",
    "\n",
    "                #length of each process's portion of the original vector\n",
    "                local_N = np.array([N//size])\n",
    "\n",
    "#communicate local array size to all processes\n",
    "comm.Bcast(local_N, root=0)\n",
    "\n",
    "#initialize as numpy arrays\n",
    "local_x = np.zeros(local_N)\n",
    "local_y = np.zeros(local_N)\n",
    "\n",
    "#divide up vectors\n",
    "comm.Scatter(x, local_x, root=0)\n",
    "comm.Scatter(y, local_y, root=0)\n",
    "\n",
    "#local computation of dot product\n",
    "local_dot = np.array([np.dot(local_x, local_y)])\n",
    "\n",
    "#sum the results of each\n",
    "comm.Reduce(local_dot, dot, op = MPI.SUM)\n",
    "\n",
    "if (rank == 0):\n",
    "                print (\"The dot product computed with MPI:\", dot[0])\n",
    "                print (\"The dot product computed w/o  MPI:\", np.dot(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "QjotqyGTHGfX",
    "outputId": "6f9cdb40-ff4f-4b17-8d30-45221a6bd56e"
   },
   "outputs": [],
   "source": [
    "!mpirun --allow-run-as-root -n 10 python mpi.py 4000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcjQGyquDOtQ"
   },
   "source": [
    "\n",
    "**Exercise:** Why is the result $\\approx$ 1E6? A bad RNG?\n",
    "\n",
    "### Reduce-based computation of $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODozaK9rDOtS"
   },
   "outputs": [],
   "source": [
    "# run syntax example: mpirun -n 10 python mpi.py 4000000\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "#read from command line\n",
    "N = int(sys.argv[1])    #number of terms\n",
    "\n",
    "#initialize as numpy array\n",
    "s = np.array([0.])\n",
    "\n",
    "#test for conformability\n",
    "if (N % size != 0):\n",
    "    print (\"the number of processors must evenly divide n.\")\n",
    "    comm.Abort()\n",
    "\n",
    "#length of each process's portion of the original vector\n",
    "local_N = np.array([N/size])\n",
    "\n",
    "def getPartialSum(start, end):\n",
    "    a = np.arange(start, end)\n",
    "    return np.sum(1./(a*a))\n",
    "\n",
    "#local computation of partial sum\n",
    "local_s = getPartialSum(1+rank*local_N, 1+(rank+1)*local_N)\n",
    "local_s = np.array([local_s])\n",
    "\n",
    "#sum the results of each local sum\n",
    "comm.Reduce(local_s, s, op = MPI.SUM)\n",
    "\n",
    "if (rank == 0):\n",
    "    pi_approx = np.sqrt(6*s[0])\n",
    "    print (\"pi_approx:\", pi_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOuSPxKJDOtl"
   },
   "source": [
    "The program execution time can be measured with commands `time` or `/usr/bin/time -v`:\n",
    "\n",
    "(`real`: wall clock time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "XIc3lRQwJFjU",
    "outputId": "81055b72-ddcc-4b12-90a7-85cd82bf6869"
   },
   "outputs": [],
   "source": [
    "!time mpirun --allow-run-as-root -n 1 python mpi.py 100000000\n",
    "!time mpirun --allow-run-as-root -n 2 python mpi.py 100000000\n",
    "!time mpirun --allow-run-as-root -n 4 python mpi.py 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLSL8ssHKvq3"
   },
   "source": [
    "Speedup and efficiency of parallelization with 2 processes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "xbMya2FYDOto",
    "outputId": "41099da0-e8f5-49e6-dca4-561cdecdea14"
   },
   "outputs": [],
   "source": [
    "Speedup = 1.589/1.365\n",
    "print ('Speedup:', Speedup)\n",
    "\n",
    "Efficiency = Speedup/2\n",
    "print ('Efficiency:', Efficiency)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MPI_2021_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
