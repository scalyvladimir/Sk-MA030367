{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zfbrvMoy9Ed",
    "outputId": "7a7e8dee-1a04-4cd7-b477-948617373a0d"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jd1axtgLy9Id"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import polara\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items\n",
    "\n",
    "from polara.lib.tensor import hooi\n",
    "from polara.lib.sparse import tensor_outer_at\n",
    "\n",
    "from sa_hooi import sa_hooi, form_attention_matrix, get_scaling_weights, generate_position_projector\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BDtwh0bdy9Mk"
   },
   "outputs": [],
   "source": [
    "def full_preproccessing():\n",
    "    data = get_movielens_data(include_time=True)\n",
    "    test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.9, interpolation='nearest'\n",
    "    )\n",
    "\n",
    "    test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "    train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    "    )\n",
    "    \n",
    "    training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "    test_data = reindex(test_data_, data_index['items'])\n",
    "\n",
    "    testset_, holdout_ = leave_one_out(\n",
    "    test_data, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "    testset_valid_, holdout_valid_ = leave_one_out(\n",
    "        testset_, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "\n",
    "    test_users_val = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "    testset_valid = testset_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "    holdout_valid = holdout_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "\n",
    "    test_users = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "    testset = testset_.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout_.query('userid in @test_users').sort_values('userid')\n",
    "\n",
    "\n",
    "    assert holdout_valid.set_index('userid')['timestamp'].ge(\n",
    "        testset_valid\n",
    "        .groupby('userid')\n",
    "        ['timestamp'].max()\n",
    "    ).all()\n",
    "\n",
    "    data_description = dict(\n",
    "        users = data_index['users'].name,\n",
    "        items = data_index['items'].name,\n",
    "        feedback = 'rating',\n",
    "        n_users = len(data_index['users']),\n",
    "        n_items = len(data_index['items']),\n",
    "        n_ratings = training['rating'].nunique(),\n",
    "        min_rating = training['rating'].min()\n",
    "        #test_users_val = holdout_valid[data_index['users'].name].drop_duplicates().values,\n",
    "        #test_users_test = holdout[data_index['users'].name].drop_duplicates().values\n",
    "    )\n",
    "\n",
    "    return training, testset_valid, holdout_valid, testset, holdout, data_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmYMpvcBy9QZ",
    "outputId": "6dfec15f-9e51-4ef5-afd6-4dc14dddf98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 113 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "training, testset_valid, holdout_valid, testset, holdout, data_description = full_preproccessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SM-pkXQx1GwO"
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "\n",
    "def tf_model_build(config, data, data_description, attention_matrix=np.array([]), exp_decay = False, decay=1):\n",
    "    userid = data_description[\"users\"]\n",
    "    itemid = data_description[\"items\"]\n",
    "    feedback = data_description[\"feedback\"]\n",
    "\n",
    "    idx = data[[userid, itemid, feedback]].values\n",
    "    idx[:, -1] = idx[:, -1] - data_description['min_rating'] # works only for integer ratings!\n",
    "    val = np.ones(idx.shape[0], dtype='f8')\n",
    "    \n",
    "    n_users = data_description[\"n_users\"]\n",
    "    n_items = data_description[\"n_items\"]\n",
    "    n_ratings = data_description[\"n_ratings\"]\n",
    "    shape = (n_users, n_items, n_ratings)\n",
    "    core_shape = config['mlrank']\n",
    "    num_iters = config[\"num_iters\"]\n",
    "    \n",
    "    if (len(attention_matrix) == 0):\n",
    "        attention_matrix = form_attention_matrix(\n",
    "            data_description['n_ratings'],\n",
    "            #config[\"attention_decay\"],\n",
    "            format = 'csr',\n",
    "            decay_factor=decay,\n",
    "            exponential_decay=exp_decay\n",
    "        )\n",
    "\n",
    "    item_popularity = (\n",
    "        data[itemid]\n",
    "        .value_counts(sort=False)\n",
    "        .reindex(range(n_items))\n",
    "        .fillna(1)\n",
    "        .values\n",
    "    )\n",
    "    scaling_weights = get_scaling_weights(item_popularity, scaling=config[\"scaling\"])\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        u0, u1, u2 = sa_hooi(\n",
    "            idx, val, shape, config[\"mlrank\"],\n",
    "            attention_matrix = attention_matrix,\n",
    "            scaling_weights = scaling_weights,\n",
    "            max_iters = config[\"num_iters\"],\n",
    "            parallel_ttm = False,\n",
    "            randomized = config[\"randomized\"],\n",
    "            growth_tol = config[\"growth_tol\"],\n",
    "            seed = config[\"seed\"],\n",
    "            iter_callback = None,\n",
    "        )\n",
    "    \n",
    "    return u0, u1, u2, attention_matrix\n",
    "        \n",
    "\n",
    "config = {\n",
    "    \"scaling\": 1,\n",
    "    \"mlrank\": (30, 30, 5),\n",
    "    \"n_ratings\": data_description['n_ratings'],\n",
    "    \"num_iters\": 5,\n",
    "    \"attention_decay\": 1,\n",
    "    \"randomized\": True,\n",
    "    \"growth_tol\": 1e-4,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "def tf_scoring(params, data, data_description, context=[\"3+4+5\"]):\n",
    "    user_factors, item_factors, feedback_factors, attention_matrix = params\n",
    "    userid = data_description[\"users\"]\n",
    "    itemid = data_description[\"items\"]\n",
    "    feedback = data_description[\"feedback\"]\n",
    "\n",
    "    data = data.sort_values(userid)\n",
    "    useridx = data[userid]\n",
    "    \n",
    "    n_users = useridx.nunique()\n",
    "    n_items = data_description['n_items']\n",
    "    n_ratings = data_description['n_ratings']\n",
    "    \n",
    "    scores = np.zeros((n_users, n_items))\n",
    "    inv_attention = np.linalg.inv(attention_matrix.A)\n",
    "    for i, u in tqdm(enumerate(np.unique(useridx))):\n",
    "        data_u = data[data.userid==u]\n",
    "        P = csr_matrix((np.ones(data_u.shape[0]), (data_u[itemid].values, data_u[feedback].values - data_description['min_rating'])), (n_items, n_ratings))\n",
    "        res = item_factors @ (item_factors.T @ (P @ (attention_matrix @ (feedback_factors @ (inv_attention.T @ feedback_factors).T))))\n",
    "        if (context == \"5\"):\n",
    "            scores[i] = np.sum(res[:, -1:], axis=1)\n",
    "        elif (context == \"4+5\"):\n",
    "            scores[i] = np.sum(res[:, -2:], axis=1)\n",
    "        elif (context == \"3+4+5\"):\n",
    "            scores[i] = np.sum(res[:, -3:], axis=1)\n",
    "        elif (context == \"2+3+4+5\"):\n",
    "            scores[i] = np.sum(res[:, -4:], axis=1)\n",
    "        elif (context == \"3+4+5-2-1\"):\n",
    "            scores[i] = np.sum(res[:, 2:], axis=1) - np.sum(res[:, :2], axis=1)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def make_prediction(tf_scores, holdout, data_description, mode, context=\"\"):\n",
    "    if (mode):\n",
    "        print(f\"for context {context} evaluation:\")\n",
    "    for n in [5, 10, 20]:\n",
    "        tf_recs = topn_recommendations(tf_scores, n)\n",
    "        hr, mrr, cov = model_evaluate(tf_recs, holdout, data_description, topn=n)\n",
    "        print(f\"{mode} : HR@{n} = {hr:.4f}, MRR@{n} == {mrr:.4f}, Coverage@{n} = {cov:.4f}\")\n",
    "        if (n == 10):\n",
    "            mrr10 = mrr\n",
    "    return mrr10\n",
    "\n",
    "def valid_mlrank(mlrank):\n",
    "    '''\n",
    "    Only allow ranks that are suitable for truncated SVD computations\n",
    "    on unfolded compressed tensor (the result of ttm product in HOOI).\n",
    "    '''\n",
    "    r1, r2, r3 = mlrank\n",
    "    return r1*r2 > r3 and r1*r3 > r2 and r2*r3 > r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3p0AyeCf3QVU"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from polara.evaluation.pipelines import random_grid\n",
    "\n",
    "def full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix, exp_decay, decay):\n",
    "    \n",
    "    config[\"mlrank\"] = (30, 30, 5)\n",
    "    \n",
    "    print(\"Starting pipeline...\")\n",
    "    print(\"Training with different context in progress...\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    best_mrr_context = \"3+4+5\"\n",
    "    best_mrr = 0.0\n",
    "    for context in [\"5\", \"4+5\", \"3+4+5\", \"2+3+4+5\", \"3+4+5-2-1\"]:\n",
    "        tf_params = tf_model_build(config, training, data_description, attention_matrix=attention_matrix, exp_decay = exp_decay, decay=decay)\n",
    "        seen_data = testset_valid\n",
    "        tf_scores = tf_scoring(tf_params, seen_data, data_description, context)\n",
    "        downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "        cur_mrr = make_prediction(tf_scores, holdout_valid, data_description, \"Validation\", context)\n",
    "        print(\"------------------------------------------------------\")\n",
    "        if (cur_mrr > best_mrr):\n",
    "            best_mrr = cur_mrr\n",
    "            best_mrr_context = context\n",
    "\n",
    "    print(f\"Tuning model with the best context influence on MRR@10 (context is {best_mrr_context})...\")\n",
    "\n",
    "    tf_hyper = {\n",
    "    'r1': range(30, 56, 5),\n",
    "    'r2': range(30, 56, 5),\n",
    "    'r3': range(5, 6, 5),\n",
    "    }\n",
    "\n",
    "    grid, param_names = random_grid(tf_hyper, n=0)\n",
    "    tf_grid = [tuple(mlrank) for mlrank in grid if valid_mlrank(mlrank)]\n",
    "\n",
    "    hr_tf = {}\n",
    "    mrr_tf = {}\n",
    "    cov_tf = {}\n",
    "    for mlrank in tqdm(tf_grid):\n",
    "        with io.capture_output() as captured:\n",
    "            config['mlrank'] = mlrank\n",
    "            tf_params = tf_model_build(config, training, data_description, attention_matrix=attention_matrix, exp_decay = exp_decay, decay=decay)\n",
    "            tf_scores = tf_scoring(tf_params, seen_data, data_description, best_mrr_context)\n",
    "            downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "            tf_recs = topn_recommendations(tf_scores, topn=10)\n",
    "            hr, mrr, cov = model_evaluate(tf_recs, holdout_valid, data_description, topn=10)\n",
    "            hr_tf[mlrank] = hr\n",
    "            mrr_tf[mlrank] = mrr\n",
    "            cov_tf[mlrank] = cov\n",
    "\n",
    "    print(f'Best HR={pd.Series(hr_tf).max():.4f} achieved with mlrank={pd.Series(hr_tf).idxmax()}')\n",
    "    print(f'Best MRR={pd.Series(mrr_tf).max():.4f} achieved with mlrank={pd.Series(mrr_tf).idxmax()}')\n",
    "    print(f'COV={pd.Series(cov_tf)[pd.Series(mrr_tf).idxmax()]:.4f} (based on best HR value)')\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"Evaluation of the best model on test holdout in progress...\")\n",
    "    \n",
    "    config[\"mlrank\"] = pd.Series(mrr_tf).idxmax()\n",
    "    tf_params = tf_model_build(config, training, data_description, attention_matrix=attention_matrix, exp_decay = exp_decay, decay=decay)\n",
    "\n",
    "    seen_data = testset\n",
    "    tf_scores = tf_scoring(tf_params, seen_data, data_description, best_mrr_context)\n",
    "    downvote_seen_items(tf_scores, seen_data, data_description)\n",
    "    cur_mrr = make_prediction(tf_scores, holdout, data_description, \"Test\", best_mrr_context)\n",
    "    print(\"Pipeline ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAJa3SH0_RNF"
   },
   "source": [
    "## Linear attention, decay factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "g90S39EK8a97",
    "outputId": "4ba7336d-062c-4466-f803-6b4071ca63e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline...\n",
      "Training with different context in progress...\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:00, 1323.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 5 evaluation:\n",
      "Validation : HR@5 = 0.0281, MRR@5 == 0.0170, Coverage@5 = 0.1073\n",
      "Validation : HR@10 = 0.0484, MRR@10 == 0.0197, Coverage@10 = 0.1513\n",
      "Validation : HR@20 = 0.0906, MRR@20 == 0.0226, Coverage@20 = 0.2132\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:00, 1217.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 4+5 evaluation:\n",
      "Validation : HR@5 = 0.0281, MRR@5 == 0.0181, Coverage@5 = 0.1062\n",
      "Validation : HR@10 = 0.0598, MRR@10 == 0.0220, Coverage@10 = 0.1488\n",
      "Validation : HR@20 = 0.1108, MRR@20 == 0.0257, Coverage@20 = 0.2036\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1115.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 3+4+5 evaluation:\n",
      "Validation : HR@5 = 0.0317, MRR@5 == 0.0184, Coverage@5 = 0.1136\n",
      "Validation : HR@10 = 0.0633, MRR@10 == 0.0225, Coverage@10 = 0.1543\n",
      "Validation : HR@20 = 0.1170, MRR@20 == 0.0261, Coverage@20 = 0.2094\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1128.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 2+3+4+5 evaluation:\n",
      "Validation : HR@5 = 0.0281, MRR@5 == 0.0180, Coverage@5 = 0.1172\n",
      "Validation : HR@10 = 0.0633, MRR@10 == 0.0227, Coverage@10 = 0.1574\n",
      "Validation : HR@20 = 0.1161, MRR@20 == 0.0263, Coverage@20 = 0.2151\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1069.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 3+4+5-2-1 evaluation:\n",
      "Validation : HR@5 = 0.0264, MRR@5 == 0.0166, Coverage@5 = 0.1128\n",
      "Validation : HR@10 = 0.0554, MRR@10 == 0.0204, Coverage@10 = 0.1527\n",
      "Validation : HR@20 = 0.1020, MRR@20 == 0.0236, Coverage@20 = 0.2105\n",
      "------------------------------------------------------\n",
      "Tuning model with the best context influence on MRR@10 (context is 2+3+4+5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [03:30<00:00,  5.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HR=0.0792 achieved with mlrank=(55, 50, 5)\n",
      "Best MRR=0.0288 achieved with mlrank=(55, 45, 5)\n",
      "COV=0.1763 (based on best HR value)\n",
      "---------------------------------------------------------\n",
      "Evaluation of the best model on test holdout in progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1013.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 2+3+4+5 evaluation:\n",
      "Test : HR@5 = 0.0299, MRR@5 == 0.0134, Coverage@5 = 0.1307\n",
      "Test : HR@10 = 0.0563, MRR@10 == 0.0167, Coverage@10 = 0.1755\n",
      "Test : HR@20 = 0.0967, MRR@20 == 0.0196, Coverage@20 = 0.2426\n",
      "Pipeline ended.\n"
     ]
    }
   ],
   "source": [
    "full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix=np.array([]), exp_decay=False, decay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHY4qoqt_aEi"
   },
   "source": [
    "## Exponential attention, decay factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaling': 1,\n",
       " 'mlrank': (55, 45, 5),\n",
       " 'n_ratings': 5,\n",
       " 'num_iters': 5,\n",
       " 'attention_decay': 1,\n",
       " 'randomized': True,\n",
       " 'growth_tol': 0.0001,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qu4qFGxU8k-w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline...\n",
      "Training with different context in progress...\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1088.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 5 evaluation:\n",
      "Validation : HR@5 = 0.0325, MRR@5 == 0.0180, Coverage@5 = 0.1191\n",
      "Validation : HR@10 = 0.0510, MRR@10 == 0.0203, Coverage@10 = 0.1739\n",
      "Validation : HR@20 = 0.0932, MRR@20 == 0.0231, Coverage@20 = 0.2297\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1099.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 4+5 evaluation:\n",
      "Validation : HR@5 = 0.0325, MRR@5 == 0.0194, Coverage@5 = 0.1150\n",
      "Validation : HR@10 = 0.0642, MRR@10 == 0.0236, Coverage@10 = 0.1659\n",
      "Validation : HR@20 = 0.1152, MRR@20 == 0.0271, Coverage@20 = 0.2237\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1136.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 3+4+5 evaluation:\n",
      "Validation : HR@5 = 0.0361, MRR@5 == 0.0215, Coverage@5 = 0.1230\n",
      "Validation : HR@10 = 0.0712, MRR@10 == 0.0260, Coverage@10 = 0.1678\n",
      "Validation : HR@20 = 0.1196, MRR@20 == 0.0292, Coverage@20 = 0.2278\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1049.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 2+3+4+5 evaluation:\n",
      "Validation : HR@5 = 0.0343, MRR@5 == 0.0215, Coverage@5 = 0.1285\n",
      "Validation : HR@10 = 0.0739, MRR@10 == 0.0265, Coverage@10 = 0.1733\n",
      "Validation : HR@20 = 0.1223, MRR@20 == 0.0298, Coverage@20 = 0.2380\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1037.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 3+4+5-2-1 evaluation:\n",
      "Validation : HR@5 = 0.0317, MRR@5 == 0.0188, Coverage@5 = 0.1180\n",
      "Validation : HR@10 = 0.0624, MRR@10 == 0.0228, Coverage@10 = 0.1662\n",
      "Validation : HR@20 = 0.1073, MRR@20 == 0.0258, Coverage@20 = 0.2239\n",
      "------------------------------------------------------\n",
      "Tuning model with the best context influence on MRR@10 (context is 2+3+4+5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [03:43<00:00,  6.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HR=0.0739 achieved with mlrank=(55, 45, 5)\n",
      "Best MRR=0.0265 achieved with mlrank=(55, 45, 5)\n",
      "COV=0.1733 (based on best HR value)\n",
      "---------------------------------------------------------\n",
      "Evaluation of the best model on test holdout in progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [00:01, 1087.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for context 2+3+4+5 evaluation:\n",
      "Test : HR@5 = 0.0237, MRR@5 == 0.0121, Coverage@5 = 0.1298\n",
      "Test : HR@10 = 0.0528, MRR@10 == 0.0160, Coverage@10 = 0.1700\n",
      "Test : HR@20 = 0.0959, MRR@20 == 0.0189, Coverage@20 = 0.2369\n",
      "Pipeline ended.\n"
     ]
    }
   ],
   "source": [
    "full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix=np.array([]), exp_decay=True, decay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZJWFtvA_fMT"
   },
   "source": [
    "## Eucledian distance attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9NOoGl7_jYO"
   },
   "outputs": [],
   "source": [
    "eucl_matrix = np.zeros((5, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        eucl_matrix[i, j] = abs(i - j) / np.exp(abs(i - j)) if i != j else 5 + 1e-2\n",
    "        \n",
    "a = np.linalg.cholesky(eucl_matrix)\n",
    "\n",
    "for i in range(5):\n",
    "    a[i, i] = 1e-5\n",
    "\n",
    "attention_matrix = csr_matrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline(config, training, data_description, testset_valid, holdout_valid, testset, holdout, attention_matrix=np.array([]), exp_decay=True, decay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating distribution attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProjectRecSysV2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
